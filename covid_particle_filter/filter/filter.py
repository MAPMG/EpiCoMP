"""
A module to implement a traditional particle filter.

The core importable element of this module is the particle_filter function.
This function uses the default particle filtering algorithm as discussed in
Murphy's 'Machine Learning: A Probabilistic Perspective' (pgs. 823-831).
"""


from covid_particle_filter.particle.SEIHR import SEIHR_generator
import numpy as np
import warnings

def particle_filter(
    h_obs,
    n_particles = 10000,
    dt = 0.1,
    gen = SEIHR_generator(100000),
    particles = None,
    s_min = None
):
    
    """Perform a particle filtration.
    
    This method filters a set of particles based on their likelihood of producing the data observed.
    Particles can either be supplied through the `particles` input or randomly generated by specifying
    both a `gen` (generator object) and `n_particles`.  Note that a generator should be sub-classed from
    the abstract generators available in the `particle` sub-module of this package.
    
    You may also specify an `s_min` - the minimum effective sample size before you employ a resampling step.
    If this variable is None (as it is by default), we default to 90% of `n_particles` or the length
    of the supplied particle list.  Please note that this should not be interpreted as a suggestion.  We strongly
    recommend tuning this hyperparameter, and we have found that a setting of ~60% is frequently sufficient.
    
    Data must be supplied to this function as a list of (time, observation target) pairs.  Note that observation targets
    could be lists, tuples, or other non-atomic entities if desired, although you would have to accomodate such extensions
    in your particle's `evaluate()` function.  The algorithm employs each particle's `step()` function to step forward to the
    in the first element of the data.  The given data is passed to the particle in the `eval_value` attribute, which allows the
    particle to update its weight based on the probability of the given particle obtaining that data.
    
    We measure the effective sample size by calculating $s_{eff} = \sum_i \frac{1}{weights_i}$.  Note that this is a measure of
    the diversity of particle weights, *not* a measure of the absolute quality of a fit.  Thus, it is possible for a model to fail
    because the priors are sufficiently misspecified that none of the simulations could have possibly produced the observations.
    This may also occur if your evaluation function is misspecified or too strict.
    
    Regardless, we resample particles (with replacement) when $s_{eff} < s_{min}$.  During this operation, we also reset the weights
    on all particles to a uniform $1/N$.  Please note that particles which are eliminated are completely forgotten.  This is
    computationally efficient, but may lead to confusion or problems if you need to produce forecasts at a variety of points in
    time.  In such cases, we strongly recommend fitting up to that point in time using this function, saving the particles, then
    feeding the fitted particles back into this algorithm along with the new data.  This operation should cause no additional
    computational overhead beyond the known additional operations that you employ.
    
    Please note that this algorithm prints the value of `s_eff` at each evaluation step so you can keep track of the quality of your fit.
    
    
    Args:
        h_obs (list): a list of tuples (t,y) that indicates the evaluation value (y) occurring
        between the previous pair's t value and the current t value (by default, hospital admissions over that period)
        n_particles (int): the number of particles to create (default 10000)
        dt (numeric): the step size for the simulation (NOT for the evaluation steps; default 0.1)
        gen (generator): a generator to construct a set of particles (default SEIHR_generator from the particle module)
        particles (list of instantiated Particle objects): a list of particles that have already been partially fitted
        (overrides the generator and continues fitting these particles)
        s_min (float): minimum effective sample size that triggers resampling (defaults to 0.9*n_particles if None/not specified)
    Returns:
        A list of n_particles particles which have survived the filtration process.
    """
    
    # Get the list of particles to fit
    if particles is not None:
        n_particles = len(particles)
        t_prev = particles[0].t
        if any([t_ <= t_prev for t_, y in h_obs]):
            warnings.warn("Dropping observations with time below current simulation time")
            h_obs = [(t_, y) for t_, y in h_obs if t_ > t_prev]
            
        particles = [p.copy() for p in particles]
        
    else:
        particles = [
            gen.generate(i) for i in range(n_particles)
        ]
        t_prev = 0
    
    if s_min is None:
        s_min = n_particles*0.9
    
    #Loop over observations
    for t, y in h_obs:
        
        #Using list comprehension; for each particle, step forward to the next evaluation point
        #and update weights based on the eval_value, which need not be atomic
        tmp_particles = [
            x.step(t, i, dt = dt, eval_value = y) for x, i in zip(particles, range(len(particles)))
        ]
        
        #Harvest the weights from all particles, then normalize to add to 1
        pre_weights = [x.w_t for x in tmp_particles]
        weights = np.array(pre_weights)/np.sum(pre_weights)
        s_eff = 1/np.sum(weights**2)
        
        for x, w in zip(tmp_particles, weights):
            x.w_t = w
            
        print(s_eff)
        
        #If effective sample size is below minimum, then perform resampling
        if s_eff < s_min:
            
            particles = np.random.choice(
                tmp_particles,
                p = weights, 
                replace = 1, 
                size = len(particles)
            )
            
            for x in particles:
                x.w_t = 1/len(particles)
                
        else:
            particles = tmp_particles
        
    return particles